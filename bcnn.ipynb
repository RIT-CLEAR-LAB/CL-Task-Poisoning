{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Training on Class Increment 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:06<00:00, 23.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 14.1313\n",
      "\n",
      "Testing after Task 1\n",
      "Accuracy on Test Set 1: 66.30%\n",
      "Accuracy on Test Set 2: 0.00%\n",
      "Accuracy on Test Set 3: 0.00%\n",
      "Accuracy on Test Set 4: 0.00%\n",
      "Accuracy on Test Set 5: 0.00%\n",
      "\n",
      "Training on Class Increment 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 14.5749\n",
      "\n",
      "Testing after Task 2\n",
      "Accuracy on Test Set 1: 48.70%\n",
      "Accuracy on Test Set 2: 62.60%\n",
      "Accuracy on Test Set 3: 0.00%\n",
      "Accuracy on Test Set 4: 0.00%\n",
      "Accuracy on Test Set 5: 0.00%\n",
      "\n",
      "Training on Class Increment 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 14.7957\n",
      "\n",
      "Testing after Task 3\n",
      "Accuracy on Test Set 1: 0.00%\n",
      "Accuracy on Test Set 2: 8.20%\n",
      "Accuracy on Test Set 3: 64.20%\n",
      "Accuracy on Test Set 4: 0.00%\n",
      "Accuracy on Test Set 5: 0.00%\n",
      "\n",
      "Training on Class Increment 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:06<00:00, 23.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 14.9370\n",
      "\n",
      "Testing after Task 4\n",
      "Accuracy on Test Set 1: 0.00%\n",
      "Accuracy on Test Set 2: 0.00%\n",
      "Accuracy on Test Set 3: 19.10%\n",
      "Accuracy on Test Set 4: 72.30%\n",
      "Accuracy on Test Set 5: 0.00%\n",
      "\n",
      "Training on Class Increment 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 14.1569\n",
      "\n",
      "Testing after Task 5\n",
      "Accuracy on Test Set 1: 0.00%\n",
      "Accuracy on Test Set 2: 0.00%\n",
      "Accuracy on Test Set 3: 0.00%\n",
      "Accuracy on Test Set 4: 62.25%\n",
      "Accuracy on Test Set 5: 64.75%\n",
      "Training and testing across all class increments completed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Define Bayesian Convolutional Layer (unchanged)\n",
    "\n",
    "# Define Bayesian Convolutional Layer to output both mu and log_sigma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Bayesian Convolutional Layer to output both mu and log_sigma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Bayesian Convolutional Layer to output both mu and log_sigma\n",
    "class BayesianConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):\n",
    "        super(BayesianConv2d, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Parameters for weight mean and log standard deviation\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).normal_(0, 0.1))\n",
    "        self.weight_log_sigma = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size).normal_(-3, 0.1))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias_mu = nn.Parameter(torch.Tensor(out_channels).normal_(0, 0.1))\n",
    "            self.bias_log_sigma = nn.Parameter(torch.Tensor(out_channels).normal_(-3, 0.1))\n",
    "        else:\n",
    "            self.bias_mu = None\n",
    "            self.bias_log_sigma = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample weights and biases\n",
    "        weight = self.weight_mu + torch.exp(self.weight_log_sigma) * torch.randn_like(self.weight_log_sigma)\n",
    "        bias = self.bias_mu + torch.exp(self.bias_log_sigma) * torch.randn_like(self.bias_log_sigma) if self.bias_mu is not None else None\n",
    "        \n",
    "        # Convolution operation\n",
    "        predicted_mu = F.conv2d(x, weight, bias, stride=self.stride, padding=self.padding)\n",
    "        predicted_log_sigma = self.weight_log_sigma.mean()  # Simplify to single log_sigma value for this layer\n",
    "        \n",
    "        # Return both mu and log_sigma\n",
    "        return predicted_mu, predicted_log_sigma\n",
    "\n",
    "# Define Bayesian Fully Connected Layer to output mu and log_sigma\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BayesianLinear, self).__init__()\n",
    "        \n",
    "        # Parameters for weight mean and log standard deviation\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.1))\n",
    "        self.weight_log_sigma = nn.Parameter(torch.Tensor(out_features, in_features).normal_(-3, 0.1))\n",
    "        \n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.1))\n",
    "        self.bias_log_sigma = nn.Parameter(torch.Tensor(out_features).normal_(-3, 0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample weights and biases\n",
    "        weight = self.weight_mu + torch.exp(self.weight_log_sigma) * torch.randn_like(self.weight_log_sigma)\n",
    "        bias = self.bias_mu + torch.exp(self.bias_log_sigma) * torch.randn_like(self.bias_log_sigma)\n",
    "        \n",
    "        # Compute the predicted mean and log_sigma\n",
    "        predicted_mu = F.linear(x, weight, bias)\n",
    "        predicted_log_sigma = self.weight_log_sigma.mean()  # Simplify to a single log sigma value\n",
    "        \n",
    "        return predicted_mu, predicted_log_sigma\n",
    "\n",
    "# Define Bayesian ResNet Block\n",
    "class BayesianBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BayesianBasicBlock, self).__init__()\n",
    "        self.conv1 = BayesianConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = BayesianConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = None\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                BayesianConv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_tuple):\n",
    "        x, prev_log_sigma = x_tuple\n",
    "        \n",
    "        # First convolutional layer\n",
    "        mu1, log_sigma1 = self.conv1(x)\n",
    "        out = F.relu(self.bn1(mu1))\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        mu2, log_sigma2 = self.conv2(out)\n",
    "        out = self.bn2(mu2)\n",
    "        \n",
    "        # Handle shortcut connection\n",
    "        if self.shortcut:\n",
    "            identity = x\n",
    "            for layer in self.shortcut:\n",
    "                if isinstance(layer, BayesianConv2d):\n",
    "                    identity, _ = layer(identity)\n",
    "                else:\n",
    "                    identity = layer(identity)\n",
    "        else:\n",
    "            identity = x\n",
    "            \n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Average the log_sigma values\n",
    "        current_log_sigma = (log_sigma1 + log_sigma2) / 2\n",
    "        if prev_log_sigma is not None:\n",
    "            current_log_sigma = (current_log_sigma + prev_log_sigma) / 2\n",
    "            \n",
    "        return out, current_log_sigma\n",
    "\n",
    "class BayesianResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(BayesianResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = BayesianConv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Calculate output size\n",
    "        temp_input = torch.randn(1, 3, 32, 32)\n",
    "        temp_output, _ = self._forward_conv_layers(temp_input)\n",
    "        self.flattened_size = temp_output.view(temp_output.size(0), -1).size(1)\n",
    "        \n",
    "        self.linear = BayesianLinear(self.flattened_size, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.ModuleList(layers)\n",
    "\n",
    "    def _forward_conv_layers(self, x):\n",
    "        mu1, log_sigma1 = self.conv1(x)\n",
    "        x = F.relu(self.bn1(mu1))\n",
    "        \n",
    "        log_sigmas = [log_sigma1]\n",
    "        current_log_sigma = log_sigma1\n",
    "        \n",
    "        for layer_blocks in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
    "            for block in layer_blocks:\n",
    "                x, current_log_sigma = block((x, current_log_sigma))\n",
    "                log_sigmas.append(current_log_sigma)\n",
    "        \n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        avg_log_sigma = sum(log_sigmas) / len(log_sigmas)\n",
    "        \n",
    "        return x, avg_log_sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, log_sigma_conv = self._forward_conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        mu, log_sigma_fc = self.linear(x)\n",
    "        combined_log_sigma = (log_sigma_conv + log_sigma_fc) / 2\n",
    "        \n",
    "        return mu, combined_log_sigma\n",
    "\n",
    "# Define BayesianResNet18 for CIFAR-10 with 10 classes\n",
    "def BayesianResNet18(num_classes=10):\n",
    "    return BayesianResNet(BayesianBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# Experience Replay Buffer (unchanged)\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, buffer_size=500):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add_samples(self, samples):\n",
    "        self.buffer.extend(samples)\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer = self.buffer[-self.buffer_size:]\n",
    "\n",
    "    def get_samples(self, batch_size):\n",
    "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "    \n",
    "\n",
    "def nll_gaussian_loss(predicted_logits, predicted_log_sigma, targets):\n",
    "    \"\"\"\n",
    "    Cross entropy loss with uncertainty\n",
    "    predicted_logits: predicted class logits (batch_size x num_classes)\n",
    "    predicted_log_sigma: predicted log standard deviation\n",
    "    targets: ground truth labels (batch_size)\n",
    "    \"\"\"\n",
    "    # Convert targets to one-hot encoding\n",
    "    num_classes = predicted_logits.size(1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes).float()\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probs = F.softmax(predicted_logits, dim=1)\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    ce_loss = F.cross_entropy(predicted_logits, targets)\n",
    "    \n",
    "    # Add uncertainty penalty\n",
    "    uncertainty_penalty = torch.mean(torch.exp(-predicted_log_sigma))\n",
    "    \n",
    "    return ce_loss + 0.1 * uncertainty_penalty\n",
    "\n",
    "def elbo_loss(predicted_logits, predicted_log_sigma, targets, model, kl_weight=1e-6):\n",
    "    \"\"\"\n",
    "    ELBO loss function for Bayesian classification\n",
    "    \"\"\"\n",
    "    # Likelihood loss (cross entropy)\n",
    "    nll = nll_gaussian_loss(predicted_logits, predicted_log_sigma, targets)\n",
    "    \n",
    "    # KL divergence regularization term\n",
    "    kl = 0.0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (BayesianLinear, BayesianConv2d)):\n",
    "            kl += torch.sum(-0.5 * torch.sum(1 + module.weight_log_sigma - \n",
    "                                           module.weight_mu**2 - \n",
    "                                           module.weight_log_sigma.exp()))\n",
    "    \n",
    "    return nll + kl_weight * kl\n",
    "\n",
    "# Training and testing adjustments for class-incremental learning\n",
    "def train_model(model, train_loader, buffer, optimizer, epochs=5, batch_size=64):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "            # Handle experience replay\n",
    "            if len(buffer.buffer) > batch_size:\n",
    "                replay_samples = buffer.get_samples(batch_size // 2)\n",
    "                replay_images, replay_labels = zip(*replay_samples)\n",
    "                replay_images = torch.stack(replay_images).cuda()\n",
    "                replay_labels = torch.tensor(replay_labels, dtype=torch.long).cuda()\n",
    "                images = torch.cat([images, replay_images], dim=0)\n",
    "                labels = torch.cat([labels, replay_labels], dim=0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get model predictions\n",
    "            predicted_logits, predicted_log_sigma = model(images)\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss_nll = nll_gaussian_loss(predicted_logits, predicted_log_sigma, labels)\n",
    "            loss = elbo_loss(predicted_logits, predicted_log_sigma, labels, model)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            predicted_logits, _ = model(images)\n",
    "            _, predicted = torch.max(predicted_logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Prepare CIFAR-10 dataset for class-incremental learning\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split CIFAR-10 into 5 class-incremental tasks (each task has 2 new classes)\n",
    "tasks = []\n",
    "test_tasks = []\n",
    "for i in range(5):\n",
    "    indices = [j for j in range(len(cifar10_dataset)) if cifar10_dataset.targets[j] in range(i * 2, (i + 1) * 2)]\n",
    "    tasks.append(Subset(cifar10_dataset, indices))\n",
    "    \n",
    "    test_indices = [j for j in range(len(test_dataset)) if test_dataset.targets[j] in range(i * 2, (i + 1) * 2)]\n",
    "    test_tasks.append(Subset(test_dataset, test_indices))\n",
    "\n",
    "# Train and test across class-incremental tasks\n",
    "model = BayesianResNet18(num_classes=10).cuda()\n",
    "buffer = ExperienceReplayBuffer(buffer_size=500)\n",
    "\n",
    "for task_id, task_data in enumerate(tasks):\n",
    "    print(f\"\\nTraining on Class Increment {task_id + 1}\")\n",
    "\n",
    "    # Prepare data loaders for training and testing\n",
    "    train_loader = DataLoader(task_data, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_tasks[task_id], batch_size=64, shuffle=False)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model on the current task\n",
    "    train_model(model, train_loader, buffer, optimizer, epochs=20, batch_size=64)\n",
    "\n",
    "    # Update experience replay buffer\n",
    "    for images, labels in train_loader:\n",
    "        buffer.add_samples([(image, label) for image, label in zip(images, labels)])\n",
    "    \n",
    "    # Test on all test sets after each task\n",
    "    print(f\"\\nTesting after Task {task_id + 1}\")\n",
    "    for i in range(5):\n",
    "        test_loader = DataLoader(test_tasks[i], batch_size=64, shuffle=False)\n",
    "        accuracy = test_model(model, test_loader)\n",
    "        print(f\"Accuracy on Test Set {i + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training and testing across all class increments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
