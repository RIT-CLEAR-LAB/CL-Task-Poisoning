{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: GEM and A-GEM cannot be used on Windows (quadprog required)\n"
     ]
    }
   ],
   "source": [
    "import numpy  # needed (don't change it)\n",
    "import importlib\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import setproctitle\n",
    "import torch\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "mammoth_path = '/home/jw7630/repos/CL-Task-Poisoning' #os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # nopep8\n",
    "sys.path.append(mammoth_path)  # nopep8\n",
    "sys.path.append(mammoth_path + '/datasets')  # nopep8\n",
    "sys.path.append(mammoth_path + '/backbone')  # nopep8\n",
    "sys.path.append(mammoth_path + '/models')  # nopep8\n",
    "\n",
    "\n",
    "from datasets import NAMES as DATASET_NAMES  # nopep8\n",
    "from datasets import ContinualDataset, get_dataset  # nopep8\n",
    "from models import get_all_models, get_model  # nopep8\n",
    "\n",
    "from utils.args import add_management_args\n",
    "from utils.best_args import best_args\n",
    "from utils.conf import set_random_seed\n",
    "from utils.continual_training import train as ctrain\n",
    "from utils.distributed import make_dp\n",
    "from utils.training import train\n",
    "\n",
    "def lecun_fix():\n",
    "    # Yann moved his website to CloudFlare. You need this now\n",
    "    from six.moves import urllib  # pyright: ignore\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "lecun_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Create a Namespace instance\n",
    "args = argparse.Namespace()\n",
    "\n",
    "args.seed=None\n",
    "args.notes=None\n",
    "args.non_verbose=0\n",
    "args.disable_log=0\n",
    "args.validation=0\n",
    "args.ignore_other_metrics=1\n",
    "args.debug_mode=0\n",
    "args.nowand=1\n",
    "args.wandb_entity='regaz'\n",
    "args.wandb_project='mammoth'\n",
    "args.dataset='seq-cifar100'\n",
    "args.model='der'\n",
    "args.lr=0.01\n",
    "args.optim_wd=0.0\n",
    "args.optim_mom=0.0\n",
    "args.optim_nesterov=0\n",
    "args.n_epochs=50\n",
    "args.batch_size=32\n",
    "args.distributed='no'\n",
    "args.poisoning_type=1\n",
    "args.poisoning_severity=1\n",
    "args.n_slots=None\n",
    "args.n_poisonings=1\n",
    "args.max_classes_per_poisoning=5\n",
    "args.sequential_poisonings=False\n",
    "args.alpha=0.5\n",
    "args.softmax_temp=2\n",
    "args.buffer_size=5000\n",
    "args.n_past_poisonings = 1\n",
    "args.classes_per_poisoning=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Create a Namespace instance\n",
    "args = argparse.Namespace()\n",
    "\n",
    "args.dataset=\"seq-cifar100-label-poisoning\"\n",
    "args.model=\"der\"\n",
    "args.n_epochs=50\n",
    "args.lr=0.1\n",
    "args.batch_size=32\n",
    "args.minibatch_size=32\n",
    "args.buffer_size=5000\n",
    "args.nowand=1\n",
    "args.non_verbose=1\n",
    "args.ignore_other_metrics=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model\n",
      "                             {si,lucir,rpc,ewc_on,xder_ce,fdr,er,derpp,agem,er_ace,mer,gss,agem_r,joint,sgd,lwf_mc,gem,xder_rpc,joint_gcl,pnn,icarl,gdumb,bic,hal,lwf,der,xder}\n",
      "                             [--load_best_args] [--seed SEED] [--notes NOTES]\n",
      "                             [--non_verbose {0,1}] [--disable_log {0,1}]\n",
      "                             [--validation {0,1}]\n",
      "                             [--ignore_other_metrics {0,1}]\n",
      "                             [--debug_mode DEBUG_MODE] [--nowand {0,1}]\n",
      "                             [--wandb_entity WANDB_ENTITY]\n",
      "                             [--wandb_project WANDB_PROJECT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa5264/Desktop/CL-Mammoth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='mammoth', allow_abbrev=False)\n",
    "    parser.add_argument('--model', type=str, required=True,\n",
    "                        help='Model name.', choices=get_all_models())\n",
    "    parser.add_argument('--load_best_args', action='store_true',\n",
    "                        help='Loads the best arguments for each method, '\n",
    "                             'dataset and memory buffer.')\n",
    "    torch.set_num_threads(4)\n",
    "    add_management_args(parser)\n",
    "    args = parser.parse_known_args()[0]\n",
    "    mod = importlib.import_module('models.' + args.model)\n",
    "\n",
    "    if args.load_best_args:\n",
    "        parser.add_argument('--dataset', type=str, required=True,\n",
    "                            choices=DATASET_NAMES,\n",
    "                            help='Which dataset to perform experiments on.')\n",
    "        if hasattr(mod, 'Buffer'):\n",
    "            parser.add_argument('--buffer_size', type=int, required=True,\n",
    "                                help='The size of the memory buffer.')\n",
    "        args = parser.parse_args()\n",
    "        if args.model == 'joint':\n",
    "            best = best_args[args.dataset]['sgd']\n",
    "        else:\n",
    "            best = best_args[args.dataset][args.model]\n",
    "        if hasattr(mod, 'Buffer'):\n",
    "            best = best[args.buffer_size]\n",
    "        else:\n",
    "            best = best[-1]\n",
    "        get_parser = getattr(mod, 'get_parser')\n",
    "        parser = get_parser()\n",
    "        to_parse = sys.argv[1:] + ['--' + k + '=' + str(v) for k, v in best.items()]\n",
    "        to_parse.remove('--load_best_args')\n",
    "        args = parser.parse_args(to_parse)\n",
    "        if args.model == 'joint' and args.dataset == 'mnist-360':\n",
    "            args.model = 'joint_gcl'\n",
    "    else:\n",
    "        get_parser = getattr(mod, 'get_parser')\n",
    "        parser = get_parser()\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_random_seed(args.seed)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    lecun_fix()\n",
    "    if args is None:\n",
    "        args = parse_args()\n",
    "\n",
    "    os.putenv(\"MKL_SERVICE_FORCE_INTEL\", \"1\")\n",
    "    os.putenv(\"NPY_MKL_FORCE_INTEL\", \"1\")\n",
    "\n",
    "    # Add uuid, timestamp and hostname for logging\n",
    "    args.conf_jobnum = str(uuid.uuid4())\n",
    "    args.conf_timestamp = str(datetime.datetime.now())\n",
    "    args.conf_host = socket.gethostname()\n",
    "    dataset = get_dataset(args)\n",
    "\n",
    "    if args.n_epochs is None and isinstance(dataset, ContinualDataset):\n",
    "        args.n_epochs = dataset.get_epochs()\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = dataset.get_batch_size()\n",
    "    if hasattr(importlib.import_module('models.' + args.model), 'Buffer') and args.minibatch_size is None:\n",
    "        args.minibatch_size = dataset.get_minibatch_size()\n",
    "\n",
    "    backbone = dataset.get_backbone()\n",
    "    loss = dataset.get_loss()\n",
    "    model = get_model(args, backbone, loss, dataset.get_transform())\n",
    "\n",
    "    if args.distributed == 'dp':\n",
    "        model.net = make_dp(model.net)\n",
    "        model.to('cuda:0')\n",
    "        args.conf_ngpus = torch.cuda.device_count()\n",
    "    elif args.distributed == 'ddp':\n",
    "        # DDP breaks the buffer, it has to be synchronized.\n",
    "        raise NotImplementedError('Distributed Data Parallel not supported yet.')\n",
    "\n",
    "    if args.debug_mode:\n",
    "        args.nowand = 1\n",
    "\n",
    "    # set job name\n",
    "    setproctitle.setproctitle('{}_{}_{}'.format(args.model, args.buffer_size if 'buffer_size' in args else 0, args.dataset))\n",
    "\n",
    "    if isinstance(dataset, ContinualDataset):\n",
    "        train(model, dataset, args)\n",
    "    else:\n",
    "        assert not hasattr(model, 'end_task') or model.NAME == 'joint_gcl'\n",
    "        ctrain(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args=None):\n",
    "    os.putenv(\"MKL_SERVICE_FORCE_INTEL\", \"1\")\n",
    "    os.putenv(\"NPY_MKL_FORCE_INTEL\", \"1\")\n",
    "\n",
    "    # Add uuid, timestamp and hostname for logging\n",
    "    args.conf_jobnum = str(uuid.uuid4())\n",
    "    args.conf_timestamp = str(datetime.datetime.now())\n",
    "    args.conf_host = socket.gethostname()\n",
    "    dataset = get_dataset(args)\n",
    "\n",
    "    if args.n_epochs is None and isinstance(dataset, ContinualDataset):\n",
    "        args.n_epochs = 2#dataset.get_epochs()\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = dataset.get_batch_size()\n",
    "    if hasattr(importlib.import_module('models.' + args.model), 'Buffer') and args.minibatch_size is None:\n",
    "        args.minibatch_size = dataset.get_minibatch_size()\n",
    "    args.n_epochs=2\n",
    "    backbone = dataset.get_backbone()\n",
    "    loss = dataset.get_loss()\n",
    "    model = get_model(args, backbone, loss, dataset.get_transform())\n",
    "\n",
    "    if args.distributed == 'dp':\n",
    "        model.net = make_dp(model.net)\n",
    "        model.to('cuda:0')\n",
    "        args.conf_ngpus = torch.cuda.device_count()\n",
    "    elif args.distributed == 'ddp':\n",
    "        # DDP breaks the buffer, it has to be synchronized.\n",
    "        raise NotImplementedError('Distributed Data Parallel not supported yet.')\n",
    "\n",
    "    if args.debug_mode:\n",
    "        args.nowand = 1\n",
    "\n",
    "    # set job name\n",
    "    setproctitle.setproctitle('{}_{}_{}'.format(args.model, args.buffer_size if 'buffer_size' in args else 0, args.dataset))\n",
    "\n",
    "    if isinstance(dataset, ContinualDataset):\n",
    "        train(model, dataset, args)\n",
    "    else:\n",
    "        assert not hasattr(model, 'end_task') or model.NAME == 'joint_gcl'\n",
    "        ctrain(args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      7\u001b[0m args\u001b[38;5;241m.\u001b[39mconf_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[1;32m      8\u001b[0m args\u001b[38;5;241m.\u001b[39mconf_host \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mgethostname()\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mn_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, ContinualDataset):\n\u001b[1;32m     12\u001b[0m     args\u001b[38;5;241m.\u001b[39mn_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;66;03m#dataset.get_epochs()\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/CL-Task-Poisoning/datasets/__init__.py:39\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset\u001b[39m(args: Namespace) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ContinualDataset:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Creates and returns a continual dataset.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    :param args: the arguments which contains the hyperparameters\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    :return: the continual dataset\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;129;01min\u001b[39;00m NAMES\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NAMES[args\u001b[38;5;241m.\u001b[39mdataset](args)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model\n",
      "                             {si,lucir,rpc,ewc_on,xder_ce,fdr,er,derpp,agem,er_ace,mer,gss,agem_r,joint,sgd,lwf_mc,gem,xder_rpc,joint_gcl,pnn,icarl,gdumb,bic,hal,lwf,der,xder}\n",
      "                             [--load_best_args] [--seed SEED] [--notes NOTES]\n",
      "                             [--non_verbose {0,1}] [--disable_log {0,1}]\n",
      "                             [--validation {0,1}]\n",
      "                             [--ignore_other_metrics {0,1}]\n",
      "                             [--debug_mode DEBUG_MODE] [--nowand {0,1}]\n",
      "                             [--wandb_entity WANDB_ENTITY]\n",
      "                             [--wandb_project WANDB_PROJECT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa5264/Desktop/CL-Mammoth/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='mammoth', allow_abbrev=False)\n",
    "    parser.add_argument('--model', type=str, required=True,\n",
    "                        help='Model name.', choices=get_all_models())\n",
    "    parser.add_argument('--load_best_args', action='store_true',\n",
    "                        help='Loads the best arguments for each method, '\n",
    "                             'dataset and memory buffer.')\n",
    "    torch.set_num_threads(4)\n",
    "    add_management_args(parser)\n",
    "    args = parser.parse_known_args()[0]\n",
    "    mod = importlib.import_module('models.' + args.model)\n",
    "\n",
    "    if args.load_best_args:\n",
    "        parser.add_argument('--dataset', type=str, required=True,\n",
    "                            choices=DATASET_NAMES,\n",
    "                            help='Which dataset to perform experiments on.')\n",
    "        if hasattr(mod, 'Buffer'):\n",
    "            parser.add_argument('--buffer_size', type=int, required=True,\n",
    "                                help='The size of the memory buffer.')\n",
    "        args = parser.parse_args()\n",
    "        if args.model == 'joint':\n",
    "            best = best_args[args.dataset]['sgd']\n",
    "        else:\n",
    "            best = best_args[args.dataset][args.model]\n",
    "        if hasattr(mod, 'Buffer'):\n",
    "            best = best[args.buffer_size]\n",
    "        else:\n",
    "            best = best[-1]\n",
    "        get_parser = getattr(mod, 'get_parser')\n",
    "        parser = get_parser()\n",
    "        to_parse = sys.argv[1:] + ['--' + k + '=' + str(v) for k, v in best.items()]\n",
    "        to_parse.remove('--load_best_args')\n",
    "        args = parser.parse_args(to_parse)\n",
    "        if args.model == 'joint' and args.dataset == 'mnist-360':\n",
    "            args.model = 'joint_gcl'\n",
    "    else:\n",
    "        get_parser = getattr(mod, 'get_parser')\n",
    "        parser = get_parser()\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_random_seed(args.seed)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    if args is None:\n",
    "        args = parse_args()\n",
    "\n",
    "    os.putenv(\"MKL_SERVICE_FORCE_INTEL\", \"1\")\n",
    "    os.putenv(\"NPY_MKL_FORCE_INTEL\", \"1\")\n",
    "\n",
    "    # Add uuid, timestamp and hostname for logging\n",
    "    args.conf_jobnum = str(uuid.uuid4())\n",
    "    args.conf_timestamp = str(datetime.datetime.now())\n",
    "    args.conf_host = socket.gethostname()\n",
    "    dataset = get_dataset(args)\n",
    "\n",
    "    if args.n_epochs is None and isinstance(dataset, ContinualDataset):\n",
    "        args.n_epochs = dataset.get_epochs()\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = dataset.get_batch_size()\n",
    "    if hasattr(importlib.import_module('models.' + args.model), 'Buffer') and args.minibatch_size is None:\n",
    "        args.minibatch_size = dataset.get_minibatch_size()\n",
    "\n",
    "    backbone = dataset.get_backbone()\n",
    "    loss = dataset.get_loss()\n",
    "    model = get_model(args, backbone, loss, dataset.get_transform())\n",
    "\n",
    "    if args.distributed == 'dp':\n",
    "        model.net = make_dp(model.net)\n",
    "        model.to('cuda:0')\n",
    "        args.conf_ngpus = torch.cuda.device_count()\n",
    "    elif args.distributed == 'ddp':\n",
    "        # DDP breaks the buffer, it has to be synchronized.\n",
    "        raise NotImplementedError('Distributed Data Parallel not supported yet.')\n",
    "\n",
    "    if args.debug_mode:\n",
    "        args.nowand = 1\n",
    "\n",
    "    # set job name\n",
    "    setproctitle.setproctitle('{}_{}_{}'.format(args.model, args.buffer_size if 'buffer_size' in args else 0, args.dataset))\n",
    "\n",
    "    if isinstance(dataset, ContinualDataset):\n",
    "        train(model, dataset, args)\n",
    "    else:\n",
    "        assert not hasattr(model, 'end_task') or model.NAME == 'joint_gcl'\n",
    "        ctrain(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
