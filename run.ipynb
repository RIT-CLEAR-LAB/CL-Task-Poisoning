{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # needed (don't change it)\n",
    "import importlib\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import setproctitle\n",
    "import torch\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from datasets import NAMES as DATASET_NAMES  # nopep8\n",
    "from datasets import ContinualDataset, get_dataset  # nopep8\n",
    "from models import get_all_models, get_model  # nopep8\n",
    "\n",
    "from utils.args import add_management_args\n",
    "from utils.best_args import best_args\n",
    "from utils.conf import set_random_seed\n",
    "from utils.continual_training import train as ctrain\n",
    "from utils.distributed import make_dp\n",
    "from utils.training import train\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "\n",
    "\n",
    "seed_value = 0\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    seed=0, \n",
    "    notes=None, \n",
    "    non_verbose=1, \n",
    "    disable_log=0, \n",
    "    validation=0, \n",
    "    ignore_other_metrics=1, \n",
    "    debug_mode=0, \n",
    "    nowand=1, \n",
    "    wandb_entity='regaz', \n",
    "    wandb_project='mammoth', \n",
    "    dataset='seq-cifar10', \n",
    "    model='er', \n",
    "    lr=0.1, \n",
    "    optim_wd=0.0, \n",
    "    optim_mom=0.0, \n",
    "    optim_nesterov=0, \n",
    "    n_epochs=50, \n",
    "    batch_size=32, \n",
    "    distributed='dp', \n",
    "    poisoning_type=1, \n",
    "    poisoning_severity=5, \n",
    "    n_slots=None, \n",
    "    n_poisonings=1, \n",
    "    max_classes_per_poisoning=0, \n",
    "    sequential_poisonings=False, \n",
    "    buffer_size=5000, \n",
    "    minibatch_size=32, \n",
    "    buffer_mode='reservoir', \n",
    "    conf_jobnum='c41d7384-6e52-4d1d-8799-7683921336e2', \n",
    "    conf_timestamp='2024-10-23 22:05:53.528238', \n",
    "    conf_host='DESKTOP-7A'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Task 1:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 2:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 3:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 4:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "Task 5:\n",
      "Trainset size: 10000 | Testset size: 2000\n",
      "First task train data shape: (10000, 32, 32, 3)\n",
      "First task test data shape: (2000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download and transform CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Convert dataset from Tensor to NumPy format\n",
    "train_data = np.array(trainset.data)  # CIFAR10 dataset's 'data' is already NumPy\n",
    "train_labels = np.array(trainset.targets)\n",
    "\n",
    "test_data = np.array(testset.data)\n",
    "test_labels = np.array(testset.targets)\n",
    "\n",
    "# Function to split the dataset based on class labels\n",
    "def get_class_indices(labels, class_range):\n",
    "    \"\"\"Return indices in the dataset that belong to the specified class range.\"\"\"\n",
    "    indices = [i for i, label in enumerate(labels) if label in class_range]\n",
    "    return indices\n",
    "\n",
    "def create_task_datasets(data, labels, task_classes):\n",
    "    \"\"\"Create task datasets split by class and return the split data and labels.\"\"\"\n",
    "    task_datasets = []\n",
    "    for class_range in task_classes:\n",
    "        indices = get_class_indices(labels, class_range)\n",
    "        task_data = data[indices]\n",
    "        task_labels = labels[indices]\n",
    "        task_datasets.append({'data': task_data, 'labels': task_labels})\n",
    "    return task_datasets\n",
    "\n",
    "# Define the class ranges for each task\n",
    "task_classes = [\n",
    "    [0, 1],\n",
    "    [2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7],\n",
    "    [8, 9]\n",
    "]\n",
    "\n",
    "# Create task datasets for both trainset and testset\n",
    "train_task_datasets = create_task_datasets(train_data, train_labels, task_classes)\n",
    "test_task_datasets = create_task_datasets(test_data, test_labels, task_classes)\n",
    "\n",
    "# Verify each task's dataset size\n",
    "for i, (train_task, test_task) in enumerate(zip(train_task_datasets, test_task_datasets)):\n",
    "    print(f\"Task {i + 1}:\")\n",
    "    print(f\"Trainset size: {train_task['data'].shape[0]} | Testset size: {test_task['data'].shape[0]}\")\n",
    "\n",
    "# Example: Access data and labels for the first train and test task\n",
    "train_task_data, train_task_labels = train_task_datasets[0]['data'], train_task_datasets[0]['labels']\n",
    "test_task_data, test_task_labels = test_task_datasets[0]['data'], test_task_datasets[0]['labels']\n",
    "\n",
    "print(\"First task train data shape:\", train_task_data.shape)\n",
    "print(\"First task test data shape:\", test_task_data.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[2 3]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[4 5]\n",
      "[6 7]\n",
      "[6 7]\n",
      "[8 9]\n",
      "[8 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    print(np.unique(train_task_datasets[i]['labels']))\n",
    "    print(np.unique(test_task_datasets[i]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft2, fftshift, ifftshift, ifft2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class dataset_transform(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = torch.from_numpy(y).type(torch.LongTensor)\n",
    "        self.transform = transform  # save the transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)#self.x.shape[0]  # return 1 as we have only one image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the augmented image\n",
    "        if self.transform:\n",
    "            x = self.transform(self.x[idx])\n",
    "        else:\n",
    "            x = self.x[idx]\n",
    "\n",
    "        return x.float(), self.y[idx]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, extra_labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "        self.origin = data\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        original_x = self.origin[index]\n",
    "\n",
    "        # Convert y and extra_y to LongTensor if they are numpy arrays\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, original_x\n",
    "\n",
    "def poison_transform(img, poisoning_strategy=0, factor=0.5):\n",
    "    # Skip poisoning if random factor condition is not met\n",
    "    if random.random() > factor:\n",
    "        return img\n",
    "    \n",
    "    if poisoning_strategy == 0:  # Random noise\n",
    "        noise = np.random.randn(*img.shape) * 0.2\n",
    "        return img + noise\n",
    "\n",
    "    elif poisoning_strategy == 1:  # Pixel permutation\n",
    "        img_flat = img.flatten()\n",
    "        np.random.shuffle(img_flat)  # Shuffle pixels randomly\n",
    "        return img_flat.reshape(img.shape)\n",
    "\n",
    "    elif poisoning_strategy == 2:  # Low frequencies\n",
    "        return apply_frequency_filter(img, mode='low')\n",
    "\n",
    "    elif poisoning_strategy == 3:  # High frequencies\n",
    "        return apply_frequency_filter(img, mode='high')\n",
    "\n",
    "    elif poisoning_strategy == 4:  # Add square\n",
    "        return add_square(img)\n",
    "\n",
    "    elif poisoning_strategy == 5:  # Vertical flip\n",
    "        return np.flipud(img)  # Flip image vertically\n",
    "\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def apply_frequency_filter(img, mode='low'):\n",
    "    # Apply Fourier transform and shift frequencies\n",
    "    img_fft = fft2(img)\n",
    "    img_fft_shifted = fftshift(img_fft)\n",
    "\n",
    "    rows, cols = img.shape[-2], img.shape[-1]\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    mask = np.zeros((rows, cols))\n",
    "\n",
    "    # Create a mask for low or high frequencies\n",
    "    if mode == 'low':\n",
    "        mask[crow - 30:crow + 30, ccol - 30:ccol + 30] = 1\n",
    "    else:  # High frequencies\n",
    "        mask[:crow - 30, :] = 1\n",
    "        mask[crow + 30:, :] = 1\n",
    "        mask[:, :ccol - 30] = 1\n",
    "        mask[:, ccol + 30:] = 1\n",
    "\n",
    "    # Apply the frequency filter and inverse transform\n",
    "    img_fft_filtered = img_fft_shifted * mask\n",
    "    img_fft_ishifted = ifftshift(img_fft_filtered)\n",
    "    img_filtered = ifft2(img_fft_ishifted).real\n",
    "\n",
    "    return img_filtered\n",
    "\n",
    "def add_square(img):\n",
    "    # Add a random square patch with random color\n",
    "    rows, cols = img.shape[-2], img.shape[-1]\n",
    "    \n",
    "    # Ensure square size doesn't exceed image size\n",
    "    if rows < 5 or cols < 5:\n",
    "        return img  # Skip if the image is too small for a square\n",
    "    \n",
    "    square_size = random.randint(5, min(10, rows, cols))  # Dynamically adjust square size\n",
    "    color = random.random()\n",
    "\n",
    "    start_row = random.randint(0, rows - square_size)\n",
    "    start_col = random.randint(0, cols - square_size)\n",
    "\n",
    "    # Set the square patch to a random color\n",
    "    img[start_row:start_row + square_size, start_col:start_col + square_size] = color\n",
    "    return img\n",
    "\n",
    "def apply_poisoning(x_train, poisoning_strategy=0, factor=1.0):\n",
    "    # Apply poisoning strategy to each image in the dataset using vectorized operation\n",
    "    return np.array([poison_transform(img, poisoning_strategy=poisoning_strategy, factor=factor) for img in x_train])\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "\n",
    "    def avg(self):\n",
    "        if self.count == 0:\n",
    "            return 0\n",
    "        return float(self.sum) / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:17:03<00:00, 92.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97 0.   0.   0.   0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:18:00<00:00, 93.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.866  0.8565 0.     0.     0.    ]\n",
      "poisoning the data with Random noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:19:02<00:00, 94.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.624  0.0745 0.4855 0.     0.    ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [04:10<1:06:33, 84.97s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from backbone.ResNet18 import resnet18\n",
    "from backbone.ResNet18_BBB import resnet18_bbb\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "TRANSFORM = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "backbone = resnet18_bbb(10)\n",
    "loss = F.cross_entropy\n",
    "\n",
    "model = get_model(args, backbone, loss,  transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), TRANSFORM]))\n",
    "# model = get_model(args, backbone, loss,  transform = transforms.Compose(\n",
    "#             [TRANSFORM]))\n",
    "print(model.__class__.__name__)\n",
    "# Set a seed for reproducibility\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "## train process\n",
    "for task, train_data in enumerate(train_task_datasets):\n",
    "    model.net.train()\n",
    "    x_train, y_train = train_data['data'], train_data['labels']\n",
    "    ##################poisoning#############################\n",
    "    \n",
    "    if task ==2:\n",
    "        poisoning_strategy ={0: 'Random noise', \n",
    "                            1: 'Pixel permutation', \n",
    "                            2: 'Low frequencies', \n",
    "                            3: 'High frequencies', \n",
    "                            4: 'Add square', \n",
    "                            5: 'Vertical flip'}\n",
    "        print('poisoning the data with {}'.format(poisoning_strategy[0]))\n",
    "        x_train_poison = []\n",
    "        for x in x_train:\n",
    "            x_poison = poison_transform(x, poisoning_strategy=0, factor=0.5)\n",
    "            x_train_poison.append(x_poison)\n",
    "        x_train_poison = np.array(x_train_poison)\n",
    "        x_train = x_train_poison\n",
    "    #####################################################\n",
    "    for epoch in tqdm.tqdm(range(50)):\n",
    "        train_dataset = dataset_transform(x_train, y_train, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ]))\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "        model.net.train()\n",
    "        loss_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            loss = model.meta_observe(inputs, labels, inputs)\n",
    "\n",
    "    # if task == 2:\n",
    "    #     break\n",
    "        \n",
    "    #eval\n",
    "    model.net.eval()\n",
    "    acc_array = np.zeros(len(test_task_datasets))\n",
    "    with torch.no_grad():\n",
    "        for task, test_data in enumerate(test_task_datasets):\n",
    "            acc = AverageMeter()\n",
    "            x_test, y_test = test_data['data'], test_data['labels']\n",
    "            test_dataset = dataset_transform(x_test, y_test, transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "            ]))\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=500, shuffle=False)\n",
    "            # acc_meter = AverageMeter()\n",
    "            for i, (inputs, labels) in enumerate(test_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model.net(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_cnt = (preds == labels).sum().item() / inputs.size(0) \n",
    "\n",
    "                acc.update(correct_cnt, inputs.size(0))\n",
    "            acc_array[task] = acc.avg()\n",
    "    print(acc_array)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.FloatTensor(50).uniform_(0, 100).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 44,  6, 91, 33, 71, 38, 50, 89, 45, 81, 60, 57, 36, 62, 43, 20, 52,\n",
      "        11, 13, 94, 64, 93, 86, 94, 90,  6, 19, 80, 18,  4, 78, 86, 30,  1, 59,\n",
      "        30, 32,  5, 48, 60,  7, 32, 90, 43,  9,  1, 88, 96, 31])\n"
     ]
    }
   ],
   "source": [
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "place_left = max(0, 50)\n",
    "print(place_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 16,  7, 16, 15])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.FloatTensor(5).uniform_(0, 20).long()\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0])\n",
      "tensor([2])\n",
      "tensor([7])\n",
      "{7: 2}\n"
     ]
    }
   ],
   "source": [
    "valid_indices = (indices < 10).long()\n",
    "print(valid_indices)\n",
    "idx_new_data = valid_indices.nonzero().squeeze(-1)\n",
    "idx_buffer   = indices[idx_new_data]\n",
    "print(idx_new_data)\n",
    "print(idx_buffer)\n",
    "idx_map = {idx_buffer[i].item(): idx_new_data[i].item() for i in range(idx_buffer.size(0))}\n",
    "print(idx_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
